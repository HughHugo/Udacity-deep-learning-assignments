{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify' + filename + '. Can you get to it with a browser!')\n",
    "    return filename\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5241, 3083, 12, 6, 195, 2, 3137, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_widow = 1\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['anarchism', 'as', 'a', 'originated', 'as', 'term', 'of', 'a']\n",
      "\n",
      "with num_skips = 4 and skip_widow = 2\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['originated', 'anarchism', 'a', 'term', 'originated', 'of', 'term', 'as']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape = (batch_size), dtype = np.float32)\n",
    "    labels = np.ndarray(shape = (batch_size, 1), dtype = np.float32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size = 8, num_skips = num_skips, skip_window = skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_widow = %d' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    #Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype = tf.int32)\n",
    "    \n",
    "    #Variables.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                                                      stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Model.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights = softmax_weights, biases = softmax_biases, \n",
    "                                                    inputs = embed, labels = train_labels, \n",
    "                                                    num_sampled = num_sampled, num_classes = vocabulary_size))\n",
    "    \n",
    "    #Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    #Compute the similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims = True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 8.339487\n",
      "Nearest to was roamed mccarthyism ductile promotion tertium competes cantor perverted\n",
      "Nearest to so patenting stow referent mccoy wilmington suffices maharashtra laterals\n",
      "Nearest to from valens mah disintegrate propagating sprengel roe pepsi regnal\n",
      "Nearest to d deo mixes ethiopia coveted mornings katydids irma hurd\n",
      "Nearest to b allyn centro headed drums siam undermine optional negotiating\n",
      "Nearest to united detainee magazine fredericksburg braveheart accompaniments homeless adepts monck\n",
      "Nearest to up bantu suprema progressive phi franken merge philo severe\n",
      "Nearest to than recruit baz dispensationalism primacy indictments lensman eclipsed timid\n",
      "Nearest to a powder ascalon hanjour aquifer practically hasbro alsos karelia\n",
      "Nearest to with tierra negotiations naaman depp sinhalese lips contemporaneous indignation\n",
      "Nearest to zero mcgill meantime broadbent dianne performed auditions hypocritical formulas\n",
      "Nearest to there climatology readmitted wick curve performance sprawl counterexample metered\n",
      "Nearest to six hayes dheas vagus barrier jacobs minima germanicus bette\n",
      "Nearest to if rnzaf partner sarabande proceeding inmates classicists absinthium ireann\n",
      "Nearest to seven schemes statutes temperate revert meinhof goi thinkpad shards\n",
      "Nearest to see romania excelling baleen patan hershey loci euphemism rooney\n",
      "Average loss at step 2000: 4.374444\n",
      "Average loss at step 4000: 3.864812\n",
      "Average loss at step 6000: 3.784929\n",
      "Average loss at step 8000: 3.688854\n",
      "Average loss at step 10000: 3.618491\n",
      "Nearest to was is were has by had competes be partito\n",
      "Nearest to so itasca patenting antichrist bmj off stow energy sizes\n",
      "Nearest to from in through valens pepsi at by inventing amar\n",
      "Nearest to d ukiyo fortify hurd invent turkish bopp mixes amid\n",
      "Nearest to b allyn and lorry iy conforms mantled slaughterhouse supervising\n",
      "Nearest to united magazine homeless gaborone detainee braveheart jumpers adepts pu\n",
      "Nearest to up philo lars bantu travel suprema cranberries progressive adr\n",
      "Nearest to than recruit glycine or baz peaked primacy ticketing counternarcotics\n",
      "Nearest to a the autonomist anoint autonegotiation oceania emissary sociological digger\n",
      "Nearest to with for in metastasis against by during chulainn concurrency\n",
      "Nearest to zero nine six seven eight five four three two\n",
      "Nearest to there it curve they peress still gathering bed counterexample\n",
      "Nearest to six eight four five seven three nine zero two\n",
      "Nearest to if proceeding rnzaf chemnitz xvii egotistical interplay reflect sarabande\n",
      "Nearest to seven eight six nine four five three zero two\n",
      "Nearest to see loci excelling mouse rift kay patan petronas hershey\n",
      "Average loss at step 12000: 3.603821\n",
      "Average loss at step 14000: 3.563814\n",
      "Average loss at step 16000: 3.407716\n",
      "Average loss at step 18000: 3.457720\n",
      "Average loss at step 20000: 3.542527\n",
      "Nearest to was is has were had became been be partito\n",
      "Nearest to so off antichrist patenting energy surpass keith itasca referent\n",
      "Nearest to from through into in pepsi at between under on\n",
      "Nearest to d b downtown unwillingness orville croydon africaine ukiyo bills\n",
      "Nearest to b d allyn subsided broncos and slaughterhouse groupoid fabric\n",
      "Nearest to united magazine adepts gaborone homeless reception detainee jumpers braveheart\n",
      "Nearest to up philo travel lars bantu driving imposes despotism catenary\n",
      "Nearest to than or recruit baz glycine ticketing mia representative shipowner\n",
      "Nearest to a any the this hanjour oceania digger halal autonegotiation\n",
      "Nearest to with in by between or for gagarin metastasis during\n",
      "Nearest to zero five six four seven nine three eight two\n",
      "Nearest to there they it he still which we curve peress\n",
      "Nearest to six eight four seven five nine three zero two\n",
      "Nearest to if when proceeding chemnitz tectonics egotistical though where interplay\n",
      "Nearest to seven eight six four nine five three zero two\n",
      "Nearest to see mouse webmaster loci ciboney roadway nr kay mort\n",
      "Average loss at step 22000: 3.499889\n",
      "Average loss at step 24000: 3.489311\n",
      "Average loss at step 26000: 3.477563\n",
      "Average loss at step 28000: 3.482194\n",
      "Average loss at step 30000: 3.502044\n",
      "Nearest to was is has had were became been when be\n",
      "Nearest to so surpass off antichrist keith limestone energy evidentiary summary\n",
      "Nearest to from into under through during between on in until\n",
      "Nearest to d b unwillingness downtown subsided morgenstern ukiyo conscripts africaine\n",
      "Nearest to b d subsided slaughterhouse lawgiver supervising broncos fabric sanh\n",
      "Nearest to united magazine gaborone adepts homeless republic detainee braveheart reception\n",
      "Nearest to up mulhouse them out philo travel driving lars catenary\n",
      "Nearest to than or recruit much spink mia protectionist baz geri\n",
      "Nearest to a the apprehend digger autonegotiation no saberhagen another interpolation\n",
      "Nearest to with between in for gagarin pohl under by metastasis\n",
      "Nearest to zero five seven eight four three six nine two\n",
      "Nearest to there they it still he this curve we often\n",
      "Nearest to six eight four seven five nine three two zero\n",
      "Nearest to if when though where although because before chemnitz forgotten\n",
      "Nearest to seven eight nine six five four three zero two\n",
      "Nearest to see moulds tlatoani ciboney skeletal mouse iberville loci conurbation\n",
      "Average loss at step 32000: 3.497682\n",
      "Average loss at step 34000: 3.491289\n",
      "Average loss at step 36000: 3.457953\n",
      "Average loss at step 38000: 3.304518\n",
      "Average loss at step 40000: 3.430430\n",
      "Nearest to was is became has were had been be partito\n",
      "Nearest to so surpass evidentiary antichrist keith sometimes off energy sammon\n",
      "Nearest to from into through in during under until between without\n",
      "Nearest to d b superstring nimitz unwillingness morgenstern xxvii laxness sparc\n",
      "Nearest to b d c fabric slaughterhouse n supervising sanh subsided\n",
      "Nearest to united republic gaborone magazine adepts homeless pu detainee natives\n",
      "Nearest to up out them catenary off mulhouse synaptic him enough\n",
      "Nearest to than or recruit sporty mia much nitrites mandates glycine\n",
      "Nearest to a the another any halal autonegotiation saberhagen ascalon no\n",
      "Nearest to with between hdi needlessly importantly backus viral pohl on\n",
      "Nearest to zero seven five eight nine six three four two\n",
      "Nearest to there they it he still which often now usually\n",
      "Nearest to six four seven eight five nine three two zero\n",
      "Nearest to if when where mutation chemnitz though menstruation is occultism\n",
      "Nearest to seven nine five eight six four three zero two\n",
      "Nearest to see tlatoani moulds list references skeletal metres nr iberville\n",
      "Average loss at step 42000: 3.431419\n",
      "Average loss at step 44000: 3.457344\n",
      "Average loss at step 46000: 3.444530\n",
      "Average loss at step 48000: 3.353763\n",
      "Average loss at step 50000: 3.384880\n",
      "Nearest to was is were has became had been be by\n",
      "Nearest to so then thus sometimes surpass wool if evidentiary keith\n",
      "Nearest to from through into under in during within until after\n",
      "Nearest to d b p sparc unwillingness laxness haven oh superstring\n",
      "Nearest to b d morgenstern c reputed ag regulus eia f\n",
      "Nearest to united gaborone republic adepts homeless fetus magazine ego natives\n",
      "Nearest to up out them off down him mulhouse synaptic regard\n",
      "Nearest to than or much but glycine recruit petomane spink sporty\n",
      "Nearest to a the another any campos autonegotiation apprehend zaire powder\n",
      "Nearest to with between gagarin hdi masefield marking backus euskal sprung\n",
      "Nearest to zero eight five seven four six nine three two\n",
      "Nearest to there they it he still we which spat who\n",
      "Nearest to six eight seven four five three nine two zero\n",
      "Nearest to if when where though although before says chemnitz menstruation\n",
      "Nearest to seven eight nine six three four five zero two\n",
      "Nearest to see tlatoani include references list iberville moulds ciboney challenges\n",
      "Average loss at step 52000: 3.435503\n",
      "Average loss at step 54000: 3.426433\n",
      "Average loss at step 56000: 3.442270\n",
      "Average loss at step 58000: 3.394718\n",
      "Average loss at step 60000: 3.394254\n",
      "Nearest to was is became had has were been when partito\n",
      "Nearest to so surpass then thus sometimes if indicating forgotten fact\n",
      "Nearest to from through within into during after birger until under\n",
      "Nearest to d b p f superstring n c david nimitz\n",
      "Nearest to b d c f n p morgenstern r eia\n",
      "Nearest to united gaborone depots adepts republic fetus mateo homeless magazine\n",
      "Nearest to up out them off him down synaptic regard homeland\n",
      "Nearest to than or much and but sporty petomane spink protectionist\n",
      "Nearest to a the another any euripides momenta apollinaire droll saberhagen\n",
      "Nearest to with between when ditto hdi among metastasis while in\n",
      "Nearest to zero five four eight seven six nine three two\n",
      "Nearest to there they it still he this we now today\n",
      "Nearest to six eight four five nine seven three two zero\n",
      "Nearest to if when where though although because menstruation cannot mutation\n",
      "Nearest to seven eight six four nine five three zero one\n",
      "Nearest to see include references tlatoani list but iberville moulds caste\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000: 3.237418\n",
      "Average loss at step 64000: 3.257193\n",
      "Average loss at step 66000: 3.405456\n",
      "Average loss at step 68000: 3.392177\n",
      "Average loss at step 70000: 3.359549\n",
      "Nearest to was is has were had became been when becomes\n",
      "Nearest to so thus then sometimes surpass rife if cai supposition\n",
      "Nearest to from through into within by and during between after\n",
      "Nearest to d b david nimitz f hurd n snacks laxness\n",
      "Nearest to b d c f regulus p r n w\n",
      "Nearest to united gaborone depots mateo adepts braveheart fetus homeless ego\n",
      "Nearest to up out off them down synaptic regard him homeland\n",
      "Nearest to than or and much petomane spink but sporty irritated\n",
      "Nearest to a another the any halal oceania campos each kerry\n",
      "Nearest to with between bracewell euskal among ditto backus needlessly kamikaze\n",
      "Nearest to zero five four eight six seven nine three two\n",
      "Nearest to there they it he we still this generally she\n",
      "Nearest to six eight four seven three nine five two zero\n",
      "Nearest to if when though where although mutation before because for\n",
      "Nearest to seven eight six nine four five three zero two\n",
      "Nearest to see include references tlatoani list according iberville guide simultaneity\n",
      "Average loss at step 72000: 3.376227\n",
      "Average loss at step 74000: 3.348186\n",
      "Average loss at step 76000: 3.322404\n",
      "Average loss at step 78000: 3.352211\n",
      "Average loss at step 80000: 3.376313\n",
      "Nearest to was is became were had has partito being been\n",
      "Nearest to so then thus rife forgotten rk stylised serialism keith\n",
      "Nearest to from through into within after in until pastors gustavo\n",
      "Nearest to d b f p david catamarans l lafayette nimitz\n",
      "Nearest to b d c f regulus ag superstring superfluous inlet\n",
      "Nearest to united gaborone mateo homeless depots adepts republic defendant fetus\n",
      "Nearest to up out off them down synaptic him mulhouse regard\n",
      "Nearest to than or but much petomane sporty and irritated axelrod\n",
      "Nearest to a another the oceania zaire any halal emissary campos\n",
      "Nearest to with between backus viral importantly when metastasis into in\n",
      "Nearest to zero six seven five eight four three nine two\n",
      "Nearest to there it they he we she still generally now\n",
      "Nearest to six five eight four seven three nine two zero\n",
      "Nearest to if when though where although before because after then\n",
      "Nearest to seven eight six nine five four three zero one\n",
      "Nearest to see references include according tlatoani guide but refers known\n",
      "Average loss at step 82000: 3.410480\n",
      "Average loss at step 84000: 3.412268\n",
      "Average loss at step 86000: 3.391337\n",
      "Average loss at step 88000: 3.351349\n",
      "Average loss at step 90000: 3.367496\n",
      "Nearest to was is became had were has been partito be\n",
      "Nearest to so thus then too strategists forgotten although after nontrivial\n",
      "Nearest to from through into preceding within during towards under on\n",
      "Nearest to d b plenum lathes snacks outbursts twelfth gag david\n",
      "Nearest to b c d r regulus shaped ag cdots f\n",
      "Nearest to united gaborone mateo depots constitution collapse adepts fetus republic\n",
      "Nearest to up out off them down him back synaptic mulhouse\n",
      "Nearest to than sporty irritated much laced or petomane serious graham\n",
      "Nearest to a another the obfuscated every any daylight zaire liz\n",
      "Nearest to with between by viral in metastasis into salom during\n",
      "Nearest to zero seven five eight six four nine three two\n",
      "Nearest to there they it we he still she generally now\n",
      "Nearest to six eight seven five four nine three zero two\n",
      "Nearest to if when where though although before after because whenever\n",
      "Nearest to seven eight six five four nine three zero one\n",
      "Nearest to see list tlatoani references include iberville according external proliferate\n",
      "Average loss at step 92000: 3.393849\n",
      "Average loss at step 94000: 3.256499\n",
      "Average loss at step 96000: 3.359871\n",
      "Average loss at step 98000: 3.243793\n",
      "Average loss at step 100000: 3.356386\n",
      "Nearest to was is became been has had were partito becomes\n",
      "Nearest to so thus too then forgotten how strategists sometimes stylised\n",
      "Nearest to from through into in during across within using turboprops\n",
      "Nearest to d b c africaine palmer montgomery gott h andorra\n",
      "Nearest to b d c hopper vasco ceilings utica guybrush superstring\n",
      "Nearest to united gaborone constitution mateo collapse depots fetus adepts republic\n",
      "Nearest to up off out them down back him synaptic haer\n",
      "Nearest to than or sporty and irritated but graham axelrod petomane\n",
      "Nearest to a another the any campos obfuscated sandworms liz yitzhak\n",
      "Nearest to with between needlessly hdi viral bracewell when including at\n",
      "Nearest to zero five eight four six seven nine three two\n",
      "Nearest to there they it he we generally now she still\n",
      "Nearest to six seven eight five four nine three zero two\n",
      "Nearest to if when though where although before while whenever because\n",
      "Nearest to seven eight six nine five four three zero two\n",
      "Nearest to see list tlatoani according external references guide nonempty magnate\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict = feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log = 'Nearest to %s' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
